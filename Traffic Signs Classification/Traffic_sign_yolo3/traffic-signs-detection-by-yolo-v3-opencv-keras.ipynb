{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚õîÔ∏è Traffic Signs Detection with YOLO v3, OpenCV and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Firstly, trained model in Darknet framework **detects Traffic Signs among 4 categories** by OpenCV dnn library.\n",
    "* Then, trained model in Keras **classifies** cut fragmets of Traffic Signs into one of **43 classes**.\n",
    "* Results are experimental, but can be used for further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì∞ Related Papers\n",
    "\n",
    "1. Sichkar V. N. **Real time detection and classification of traffic signs based on YOLO version 3 algorithm.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2020, vol. 20, no. 3, pp. 418‚Äì424. DOI: 10.17586/2226-1494-2020-20-3-418-424 (Full-text available on ResearchGate here: [Real time detection and classification of traffic signs based on YOLO version 3 algorithm](https://www.researchgate.net/publication/342638954_Real_time_detection_and_classification_of_traffic_signs_based_on_YOLO_version_3_algorithm)\n",
    "\n",
    "1. Sichkar V. N. **Effect of various dimension convolutional layer filters on traffic sign classification accuracy.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2019, vol. 19, no. 3, pp. 546‚Äì552. DOI: 10.17586/2226-1494-2019-19-3-546-552 (Full-text available on ResearchGate here: [Effect of various dimension convolutional layer filters on traffic sign classification accuracy](https://www.researchgate.net/publication/334074308_Effect_of_various_dimension_convolutional_layer_filters_on_traffic_sign_classification_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Related course for detection tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training YOLO v3 for Objects Detection with Custom Data.** *Build your own detector by labelling, training and testing on image, video and in real time with camera.* **Join here:** [https://www.udemy.com/course/training-yolo-v3-for-objects-detection-with-custom-data/](https://www.udemy.com/course/training-yolo-v3-for-objects-detection-with-custom-data/?referralCode=A283956A57327E37DDAD)\n",
    "\n",
    "Example of detections on video are shown below. **Trained weights** can be found in the course mentioned above.\n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fbcdae0b57021d6ac3e86a9aa2e8c4b08%2Fts_detections.gif?generation=1581700736851192&alt=media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Related course for classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Design**, **Train** & **Test** deep CNN for Image Classification.\n",
    "\n",
    "**Join** the course & enjoy new opportunities to get deep learning skills:\n",
    "\n",
    "\n",
    "[https://www.udemy.com/course/convolutional-neural-networks-for-image-classification/](https://www.udemy.com/course/convolutional-neural-networks-for-image-classification/?referralCode=12EE0D74A405BF4DDE9B)\n",
    "\n",
    "\n",
    "![](https://github.com/sichkar-valentyn/1-million-images-for-Traffic-Signs-Classification-tasks/blob/main/images/slideshow_classification.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-26T13:33:50.698128Z",
     "iopub.status.busy": "2021-10-26T13:33:50.697678Z",
     "iopub.status.idle": "2021-10-26T13:33:57.862517Z",
     "shell.execute_reply": "2021-10-26T13:33:57.861780Z",
     "shell.execute_reply.started": "2021-10-26T13:33:50.698065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['traffic-signs-classification-with-cnn', 'traffic-signs-preprocessed', 'trained-traffic-signs-detector-based-on-yolo-v3']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('../input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "print(os.listdir('C:\\\\Users\\\\codi\\\\Ex#2 traffic sign\\\\input'))\n",
    "\n",
    "# Any results we write to the current directory are saved as output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ Loading *labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:34:13.513179Z",
     "iopub.status.busy": "2021-10-26T13:34:13.512826Z",
     "iopub.status.idle": "2021-10-26T13:34:13.559339Z",
     "shell.execute_reply": "2021-10-26T13:34:13.558587Z",
     "shell.execute_reply.started": "2021-10-26T13:34:13.513126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ClassId              SignName\n",
      "0        0  Speed limit (20km/h)\n",
      "1        1  Speed limit (30km/h)\n",
      "2        2  Speed limit (50km/h)\n",
      "3        3  Speed limit (60km/h)\n",
      "4        4  Speed limit (70km/h)\n",
      "\n",
      "Speed limit (20km/h)\n",
      "Speed limit (30km/h)\n"
     ]
    }
   ],
   "source": [
    "# Reading csv file with labels' names\n",
    "# Loading two columns [0, 1] into Pandas dataFrame\n",
    "labels = pd.read_csv('C:\\\\Users\\\\codi\\\\Ex#2 traffic sign\\\\input\\\\traffic-signs-preprocessed\\\\label_names.csv')\n",
    "\n",
    "# Check point\n",
    "# Showing first 5 rows from the dataFrame\n",
    "print(labels.head())\n",
    "print()\n",
    "\n",
    "# To locate by class number use one of the following\n",
    "# ***.iloc[0][1] - returns element on the 0 column and 1 row\n",
    "print(labels.iloc[0][1])  # Speed limit (20km/h)\n",
    "# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\n",
    "print(labels['SignName'][1]) # Speed limit (30km/h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ Loading trained Keras CNN model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:34:36.196763Z",
     "iopub.status.busy": "2021-10-26T13:34:36.196396Z",
     "iopub.status.idle": "2021-10-26T13:34:39.911922Z",
     "shell.execute_reply": "2021-10-26T13:34:39.911012Z",
     "shell.execute_reply.started": "2021-10-26T13:34:36.196711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\n",
    "model = load_model('C:\\\\Users\\\\codi\\\\Ex#2 traffic sign\\\\input\\\\traffic-signs-classification-with-cnn\\\\model-23x23.h5')\n",
    "\n",
    "# Loading mean image to use for preprocessing further\n",
    "# Opening file for reading in binary mode\n",
    "with open('C:\\\\Users\\\\codi\\\\Ex#2 traffic sign\\\\input\\\\traffic-signs-preprocessed\\\\mean_image_rgb.pickle', 'rb') as f:\n",
    "    mean = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "    \n",
    "print(mean['mean_image_rgb'].shape)  # (3, 32, 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí† Loading YOLO v3 network by OpenCV dnn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading *trained weights* and *cfg file* into the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:41:40.831346Z",
     "iopub.status.busy": "2021-10-26T13:41:40.830960Z",
     "iopub.status.idle": "2021-10-26T13:41:43.650241Z",
     "shell.execute_reply": "2021-10-26T13:41:43.649353Z",
     "shell.execute_reply.started": "2021-10-26T13:41:40.831294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trained weights can be found in the course mentioned above\n",
    "path_to_weights = 'C:\\\\Users\\\\codi\\\\Ex#2 traffic sign\\\\input\\\\trained-traffic-signs-detector-based-on-yolo-v3\\\\yolov3_ts_train_5000.weights'\n",
    "path_to_cfg = 'C:\\\\Users\\\\codi\\\\Ex#2 traffic sign\\\\input\\\\trained-traffic-signs-detector-based-on-yolo-v3\\\\yolov3_ts_test.cfg'\n",
    "\n",
    "# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\n",
    "network = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n",
    "\n",
    "# To use with GPU\n",
    "network.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "network.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting *output layers* where detections are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:41:50.301691Z",
     "iopub.status.busy": "2021-10-26T13:41:50.301345Z",
     "iopub.status.idle": "2021-10-26T13:41:50.309186Z",
     "shell.execute_reply": "2021-10-26T13:41:50.308186Z",
     "shell.execute_reply.started": "2021-10-26T13:41:50.301644Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "# Getting names of all YOLO v3 layers\n",
    "layers_all = network.getLayerNames()\n",
    "\n",
    "# Check point\n",
    "#print(layers_all)\n",
    "\n",
    "# Getting only detection YOLO v3 layers that are 82, 94 and 106\n",
    "layers_names_output = [layers_all[i - 1] for i in network.getUnconnectedOutLayers()]\n",
    "\n",
    "# Check point\n",
    "print()\n",
    "print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting *probability*, *threshold* and *colour* for bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:41:56.837151Z",
     "iopub.status.busy": "2021-10-26T13:41:56.836733Z",
     "iopub.status.idle": "2021-10-26T13:41:56.846068Z",
     "shell.execute_reply": "2021-10-26T13:41:56.845030Z",
     "shell.execute_reply.started": "2021-10-26T13:41:56.837100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(43, 3)\n",
      "[ 32 240  80]\n"
     ]
    }
   ],
   "source": [
    "# Minimum probability to eliminate weak detections\n",
    "probability_minimum = 0.2\n",
    "\n",
    "# Setting threshold to filtering weak bounding boxes by non-maximum suppression\n",
    "threshold = 0.2\n",
    "\n",
    "# Generating colours for bounding boxes\n",
    "# randint(low, high=None, size=None, dtype='l')\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "# Check point\n",
    "print(type(colours))  # <class 'numpy.ndarray'>\n",
    "print(colours.shape)  # (43, 3)\n",
    "print(colours[0])  # [25  65 200]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¨ Reading input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:44:45.175900Z",
     "iopub.status.busy": "2021-10-26T13:44:45.175555Z",
     "iopub.status.idle": "2021-10-26T13:44:45.241620Z",
     "shell.execute_reply": "2021-10-26T13:44:45.240833Z",
     "shell.execute_reply.started": "2021-10-26T13:44:45.175851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading video from a file by VideoCapture object\n",
    "video = cv2.VideoCapture('C:\\\\Users\\\\codi\\\\Ex#2 traffic sign\\\\input\\\\trained-traffic-signs-detector-based-on-yolo-v3\\\\traffic-sign-to-test.mp4')\n",
    "# video = cv2.VideoCapture('../input/videofortesting/ts_video_1.mp4')\n",
    "#video = cv2.VideoCapture('../input/videofortesting/ts_video_6.mp4')\n",
    "\n",
    "# Writer that will be used to write processed frames\n",
    "if (video.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "\n",
    "writer = None\n",
    "\n",
    "# Variables for spatial dimensions of the frames\n",
    "h, w = None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ûø Processing frames in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:44:50.820051Z",
     "iopub.status.busy": "2021-10-26T13:44:50.819589Z",
     "iopub.status.idle": "2021-10-26T13:45:21.490575Z",
     "shell.execute_reply": "2021-10-26T13:45:21.489659Z",
     "shell.execute_reply.started": "2021-10-26T13:44:50.819990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame number 1 took 5.57741 seconds\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Frame number 2 took 0.52841 seconds\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Frame number 3 took 0.36402 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 4 took 0.36429 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 5 took 0.37010 seconds\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Frame number 6 took 0.36509 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 7 took 0.36414 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 8 took 0.36410 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 9 took 0.36515 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 10 took 0.36635 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 11 took 0.36928 seconds\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 12 took 0.36510 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 13 took 0.36608 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 14 took 0.36762 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 15 took 0.36763 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 16 took 0.36411 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 17 took 0.36709 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 18 took 0.36566 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 19 took 0.36511 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 20 took 0.36625 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 21 took 0.36609 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 22 took 0.36309 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 23 took 0.36609 seconds\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Frame number 24 took 0.36418 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 25 took 0.36708 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 26 took 0.36590 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 27 took 0.36453 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Frame number 28 took 0.36608 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Frame number 29 took 0.36809 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 30 took 0.36608 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 31 took 0.36609 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 32 took 0.36610 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 33 took 0.36670 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 34 took 0.36610 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 35 took 0.36930 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 36 took 0.36531 seconds\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 37 took 0.36610 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 38 took 0.36809 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Frame number 39 took 0.36510 seconds\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 40 took 0.36858 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 41 took 0.37009 seconds\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 42 took 0.36622 seconds\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Frame number 43 took 0.36909 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 44 took 0.36510 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 45 took 0.36713 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 46 took 0.36409 seconds\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 47 took 0.36913 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 48 took 0.36684 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Frame number 49 took 0.37067 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Frame number 50 took 0.36404 seconds\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Frame number 51 took 0.36510 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Frame number 52 took 0.37176 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 53 took 0.36813 seconds\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 54 took 0.36809 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Frame number 55 took 0.36993 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Frame number 56 took 0.36735 seconds\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of plots\n",
    "plt.rcParams['figure.figsize'] = (3, 3)\n",
    "\n",
    "# Variable for counting total amount of frames\n",
    "f = 0\n",
    "\n",
    "# Variable for counting total processing time\n",
    "t = 0\n",
    "\n",
    "# Catching frames in the loop\n",
    "while True:\n",
    "    # Capturing frames one-by-one\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # If the frame was not retrieved\n",
    "    if not ret:\n",
    "        break\n",
    "       \n",
    "    # Getting spatial dimensions of the frame for the first time\n",
    "    if w is None or h is None:\n",
    "        # Slicing two elements from tuple\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "    # Blob from current frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # Forward pass with blob through output layers\n",
    "    network.setInput(blob)\n",
    "    start = time.time()\n",
    "    output_from_network = network.forward(layers_names_output)\n",
    "    end = time.time()\n",
    "\n",
    "    # Increasing counters\n",
    "    f += 1\n",
    "    t += end - start\n",
    "\n",
    "    # Spent time for current frame\n",
    "    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n",
    "\n",
    "    # Lists for detected bounding boxes, confidences and class's number\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "\n",
    "    # Going through all output layers after feed forward pass\n",
    "    for result in output_from_network:\n",
    "        # Going through all detections from current output layer\n",
    "        for detected_objects in result:\n",
    "            # Getting 80 classes' probabilities for current detected object\n",
    "            scores = detected_objects[5:]\n",
    "            # Getting index of the class with the maximum value of probability\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting value of probability for defined class\n",
    "            confidence_current = scores[class_current]\n",
    "\n",
    "            # Eliminating weak predictions by minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial frame size\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Getting top left corner coordinates\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)\n",
    "                \n",
    "\n",
    "    # Implementing non-maximum suppression of given bounding boxes\n",
    "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "    # Checking if there is any detected object been left\n",
    "    if len(results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in results.flatten():\n",
    "            # Bounding box coordinates, its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "            \n",
    "            \n",
    "            # Cut fragment with Traffic Sign\n",
    "            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n",
    "            # print(c_ts.shape)\n",
    "            \n",
    "            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "                pass\n",
    "            else:\n",
    "                # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n",
    "                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "                # plt.imshow(blob_ts[0, :, :, :])\n",
    "                # plt.show()\n",
    "\n",
    "                # Feeding to the Keras CNN model to get predicted label among 43 classes\n",
    "                scores = model.predict(blob_ts)\n",
    "\n",
    "                # Scores is given for image with 43 numbers of predictions for each class\n",
    "                # Getting only one class with maximum value\n",
    "                prediction = np.argmax(scores)\n",
    "                # print(labels['SignName'][prediction])\n",
    "\n",
    "\n",
    "                # Colour for current bounding box\n",
    "                colour_box_current = colours[class_numbers[i]].tolist()\n",
    "\n",
    "                # Drawing bounding box on the original current frame\n",
    "                cv2.rectangle(frame, (x_min, y_min),\n",
    "                              (x_min + box_width, y_min + box_height),\n",
    "                              colour_box_current, 2)\n",
    "\n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n",
    "                                                       confidences[i])\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n",
    "\n",
    "\n",
    "    # Initializing writer only once\n",
    "    if writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        # Writing current processed frame into the video file\n",
    "        writer = cv2.VideoWriter('result.mp4', fourcc, 25,\n",
    "                                 (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # Write processed current frame to the file\n",
    "    writer.write(frame)\n",
    "\n",
    "\n",
    "# Releasing video reader and writer\n",
    "video.release()\n",
    "writer.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ FPS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:46:27.909639Z",
     "iopub.status.busy": "2021-10-26T13:46:27.909247Z",
     "iopub.status.idle": "2021-10-26T13:46:27.916955Z",
     "shell.execute_reply": "2021-10-26T13:46:27.915822Z",
     "shell.execute_reply.started": "2021-10-26T13:46:27.909587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames 56\n",
      "Total amount of time 25.90042 seconds\n",
      "FPS: 2.2\n"
     ]
    }
   ],
   "source": [
    "print('Total number of frames', f)\n",
    "print('Total amount of time {:.5f} seconds'.format(t))\n",
    "print('FPS:', round((f / t), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T13:46:38.076423Z",
     "iopub.status.busy": "2021-10-26T13:46:38.076051Z",
     "iopub.status.idle": "2021-10-26T13:46:38.084750Z",
     "shell.execute_reply": "2021-10-26T13:46:38.083540Z",
     "shell.execute_reply.started": "2021-10-26T13:46:38.076362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='result.mp4' target='_blank'>result.mp4</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\codi\\Ex#2 traffic sign\\result.mp4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving locally without committing\n",
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('result.mp4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéà Reading input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T17:28:00.020499Z",
     "iopub.status.busy": "2021-06-14T17:28:00.020111Z",
     "iopub.status.idle": "2021-06-14T17:28:00.058144Z",
     "shell.execute_reply": "2021-06-14T17:28:00.057331Z",
     "shell.execute_reply.started": "2021-06-14T17:28:00.020446Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m image_BGR \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/videofortesting/ts_final_1.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Check point\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Showing image shape\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mimage_BGR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)  \u001b[38;5;66;03m# tuple of (731, 1092, 3)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Getting spatial dimension of input image\u001b[39;00m\n\u001b[0;32m     19\u001b[0m h, w \u001b[38;5;241m=\u001b[39m image_BGR\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Slicing from tuple only first two elements\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Reading image with OpenCV library\n",
    "# In this way image is opened already as numpy array\n",
    "# WARNING! OpenCV by default reads images in BGR format\n",
    "# image_BGR = cv2.imread('../input/videofortesting/traffic_sign.jpg')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_1.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_2.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_3.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_4.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_1.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_2.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_3.png')\n",
    "image_BGR = cv2.imread('../input/videofortesting/ts_final_1.png')\n",
    "\n",
    "# Check point\n",
    "# Showing image shape\n",
    "print('Image shape:', image_BGR.shape)  # tuple of (731, 1092, 3)\n",
    "\n",
    "# Getting spatial dimension of input image\n",
    "h, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n",
    "\n",
    "# Check point\n",
    "# Showing height an width of image\n",
    "print('Image height={0} and width={1}'.format(h, w))  # 731 1092\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßø Processing single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T17:28:11.251577Z",
     "iopub.status.busy": "2021-06-14T17:28:11.25118Z",
     "iopub.status.idle": "2021-06-14T17:28:11.701424Z",
     "shell.execute_reply": "2021-06-14T17:28:11.700687Z",
     "shell.execute_reply.started": "2021-06-14T17:28:11.251528Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variable for counting total processing time\n",
    "t = 0\n",
    "\n",
    "# Blob from current frame\n",
    "blob = cv2.dnn.blobFromImage(image_BGR, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "# Forward pass with blob through output layers\n",
    "network.setInput(blob)\n",
    "start = time.time()\n",
    "output_from_network = network.forward(layers_names_output)\n",
    "end = time.time()\n",
    "\n",
    "# Time\n",
    "t += end - start\n",
    "print('Total amount of time {:.5f} seconds'.format(t))\n",
    "\n",
    "# Lists for detected bounding boxes, confidences and class's number\n",
    "bounding_boxes = []\n",
    "confidences = []\n",
    "class_numbers = []\n",
    "\n",
    "# Going through all output layers after feed forward pass\n",
    "for result in output_from_network:\n",
    "    # Going through all detections from current output layer\n",
    "    for detected_objects in result:\n",
    "        # Getting 80 classes' probabilities for current detected object\n",
    "        scores = detected_objects[5:]\n",
    "        # Getting index of the class with the maximum value of probability\n",
    "        class_current = np.argmax(scores)\n",
    "        # Getting value of probability for defined class\n",
    "        confidence_current = scores[class_current]\n",
    "\n",
    "        # Eliminating weak predictions by minimum probability\n",
    "        if confidence_current > probability_minimum:\n",
    "            # Scaling bounding box coordinates to the initial frame size\n",
    "            box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "            # Getting top left corner coordinates\n",
    "            x_center, y_center, box_width, box_height = box_current\n",
    "            x_min = int(x_center - (box_width / 2))\n",
    "            y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "            # Adding results into prepared lists\n",
    "            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "            confidences.append(float(confidence_current))\n",
    "            class_numbers.append(class_current)\n",
    "                \n",
    "\n",
    "# Implementing non-maximum suppression of given bounding boxes\n",
    "results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "# Checking if there is any detected object been left\n",
    "if len(results) > 0:\n",
    "    # Going through indexes of results\n",
    "    for i in results.flatten():\n",
    "        # Bounding box coordinates, its width and height\n",
    "        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "            \n",
    "            \n",
    "        # Cut fragment with Traffic Sign\n",
    "        c_ts = image_BGR[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n",
    "        # print(c_ts.shape)\n",
    "            \n",
    "        if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "            pass\n",
    "        else:\n",
    "            # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "            blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "            blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n",
    "            blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "            # plt.imshow(blob_ts[0, :, :, :])\n",
    "            # plt.show()\n",
    "\n",
    "            # Feeding to the Keras CNN model to get predicted label among 43 classes\n",
    "            scores = model.predict(blob_ts)\n",
    "\n",
    "            # Scores is given for image with 43 numbers of predictions for each class\n",
    "            # Getting only one class with maximum value\n",
    "            prediction = np.argmax(scores)\n",
    "            print(labels['SignName'][prediction])\n",
    "\n",
    "\n",
    "            # Colour for current bounding box\n",
    "            colour_box_current = colours[class_numbers[i]].tolist()\n",
    "            \n",
    "            # Green BGR\n",
    "            colour_box_current = [0, 255, 61]\n",
    "            \n",
    "            # Yellow BGR\n",
    "#             colour_box_current = [0, 255, 255]\n",
    "\n",
    "            # Drawing bounding box on the original current frame\n",
    "            cv2.rectangle(image_BGR, (x_min, y_min),\n",
    "                              (x_min + box_width, y_min + box_height),\n",
    "                              colour_box_current, 6)\n",
    "\n",
    "#             # Preparing text with label and confidence for current bounding box\n",
    "#             text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n",
    "#                                                    confidences[i])\n",
    "            \n",
    "#             # Putting text with label and confidence on the original image\n",
    "#             cv2.putText(image_BGR, text_box_current, (x_min, y_min - 15),\n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n",
    "            \n",
    "            if prediction == 5:\n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format('Speed limit 60', confidences[i])\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(image_BGR, text_box_current, (x_min - 110, y_min - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n",
    "                \n",
    "            elif prediction == 9:            \n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format('No overtaking', confidences[i])\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(image_BGR, text_box_current, (x_min - 110, y_min + box_height + 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n",
    "\n",
    "#             elif prediction == 17:            \n",
    "#                 # Preparing text with label and confidence for current bounding box\n",
    "#                 text_box_current = '{}: {:.4f}'.format('No entry', confidences[i])\n",
    "\n",
    "#                 # Putting text with label and confidence on the original image\n",
    "#                 cv2.putText(image_BGR, text_box_current, (x_min - 170, y_min - 15),\n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n",
    "                \n",
    "                \n",
    "# Saving image\n",
    "cv2.imwrite('result.png', image_BGR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶û Showing processed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T17:29:43.810504Z",
     "iopub.status.busy": "2021-06-14T17:29:43.810122Z",
     "iopub.status.idle": "2021-06-14T17:29:44.676631Z",
     "shell.execute_reply": "2021-06-14T17:29:44.675579Z",
     "shell.execute_reply.started": "2021-06-14T17:29:43.810452Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (35.0, 35.0) # Setting default size of plots\n",
    "\n",
    "image_BGR = cv2.imread('/kaggle/working/result.png')\n",
    "\n",
    "# Showing image shape\n",
    "print('Image shape:', image_BGR.shape)  # tuple of (800, 1360, 3)\n",
    "\n",
    "# Getting spatial dimension of input image\n",
    "h, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n",
    "\n",
    "# Showing height an width of image\n",
    "print('Image height={0} and width={1}'.format(h, w))  # 800 1360\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "# plt.title('Keras Visualization', fontsize=18)\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T17:30:06.810993Z",
     "iopub.status.busy": "2021-06-14T17:30:06.810643Z",
     "iopub.status.idle": "2021-06-14T17:30:06.818201Z",
     "shell.execute_reply": "2021-06-14T17:30:06.817207Z",
     "shell.execute_reply.started": "2021-06-14T17:30:06.810943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving locally without committing\n",
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('result.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé Example of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fa57f58b38e3caab6fbf72169895f5074%2Fresult.gif?generation=1585955236302060&alt=media)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
